{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9bf8d4d",
   "metadata": {},
   "source": [
    "(classicsvm)=\n",
    "\n",
    "# Классический SVM\n",
    "\n",
    "## Описание лекции\n",
    "\n",
    "В этой лекции мы рассмотрим классический метод опорных векторов (SVM) и разберем стоящую за ним математику. С согласия Евгения Соколова, мы во многом переиспользуем конспекты лекций [курса](https://github.com/esokolov/ml-course-hse) \"Машинное обучение\", читаемого на ФКН ВШЭ.\n",
    "\n",
    "План лекции такой:\n",
    "\n",
    "- Линейная классификация\n",
    "- Интуиция метода опорных векторов\n",
    "- Метод опорных векторов для линейно-разделимой выборки\n",
    "- Метод опорных векторов для линейно-неразделимой выборки\n",
    "- Решение задачи метода опорных векторов и ядровый переход\n",
    "- Плюсы и минусы SVM\n",
    "\n",
    "## Линейная классификация\n",
    "\n",
    "Основная идея линейного классификатора заключается в том, что признаковое пространство может быть разделено гиперплоскостью на два полупространства, в каждом из которых прогнозируется одно из двух значений целевого класса. Если это можно сделать без ошибок, то обучающая выборка называется _линейно разделимой_.\n",
    "\n",
    "```{figure} /_static/qsvmblock/classicsvm/lin_classifier.png\n",
    ":width: 450px\n",
    ":name: lin_classifier\n",
    "\n",
    "Линейная классификация\n",
    "```\n",
    "\n",
    "Рассмотрим задачу бинарной классификации, в которой $\\mathbb{X} = \\mathbb{R}^d$ -- пространство объектов,\n",
    "$Y = \\{-1, +1\\}$ -- множество допустимых ответов (целевой признак),\n",
    "$X = \\{(x_i, y_i)\\}_{i = 1}^\\ell$ -- обучающая выборка. Будем класс $+1$ называть положительным, а класс $-1$ -- отрицательным. Здесь $d$ -- размерность признакового пространства, $\\ell$ -- количество примеров в обучающей выборке.\n",
    "\n",
    "__Линейная модель классификации__ определяется следующим образом:\n",
    "\n",
    "$$\n",
    "    a(x) =\n",
    "    sign \\left(\n",
    "        \\langle w, x \\rangle + b\n",
    "    \\right)\n",
    "    =\n",
    "    sign \\left(\n",
    "        \\sum_{j = 1}^{d} w_j x_j + b\n",
    "    \\right),\n",
    "$$\n",
    "\n",
    "где $w \\in \\mathbb{R}^d$ -- вектор весов, $b \\in \\mathbb{R}$ -- сдвиг (bias), $sign(\\bullet)$ – функция \"сигнум\", возвращающая знак своего аргумента, $a(x)$ – ответ классификатора на примере $x$.\n",
    "\n",
    "Часто считают, что среди признаков\n",
    "есть константа, $x_{d + 1} = 1$.\n",
    "В этом случае нет необходимости вводить сдвиг $b$,\n",
    "и линейный классификатор можно задавать как\n",
    "\n",
    "$$\n",
    "    a(x) = sign \\langle w, x \\rangle.\n",
    "$$\n",
    "\n",
    "Геометрически линейный классификатор соответствует гиперплоскости с вектором нормали $w$.\n",
    "Величина скалярного произведения $\\langle w, x \\rangle$ пропорциональна расстоянию от гиперплоскости до точки $x$, а его знак показывает, с какой стороны от гиперплоскости находится\n",
    "данная точка. Таким образом, линейный классификатор разделяет пространство на две части с помощью гиперплоскости, и при этом одно полупространство относит к положительному классу, а другое -- к отрицательному.\n",
    "\n",
    "Пожалуй, самый известный и популярный на практике представитель семейства линейных классификаторов -- логистическая регрессия. На русском языке про нее можно почитать в [статье](https://habr.com/ru/company/ods/blog/323890/) открытого курса по машинному обучению или в упомянутых [лекциях](https://github.com/esokolov/ml-course-hse/tree/master/2021-fall/lecture-notes) Евгения Соколова. В этих лекциях также объясняется, как происходит обучение модели (подбор весов $w$) за счет минимизации функции потерь.\n",
    "\n",
    "## Интуиция метода опорных векторов\n",
    "\n",
    "Метод опорных векторов (Support Vector Machine, SVM) основан на идее максимизации отступа между классами. Пока не вводим этот термин формально, но передадим интуицию метода. На Рис. {numref}`linclass_margins` показана линейно-разделимая выборка, кружки соответствуют положительным примерам, а квадраты -- отрицательным (или наоборот), а оси -- некоторым признакам этих примеров. На рисунке слева показаны две прямые (в общем случае -- гиперплоскости), разделяющие выборку. Кажется, что синяя прямая лучше тем, что она дальше отстоит от примеров обучающей выборки, чем красная прямая (отступ -- больше), и потому лучше будет разделять другие примеры из того же распределения, что и примеры обучающей выборки. То есть такой линейный классификатор будет лучше обобщаться на новые данные. Теория подтверждает описанную интуицию {cite}`MohriRostamizadehTalwalkar18`.\n",
    "\n",
    "\n",
    "```{figure} /_static/qsvmblock/classicsvm/linclass_margins.png\n",
    ":width: 450px\n",
    ":name: linclass_margins\n",
    "\n",
    "Слева показаны две прямые (в общем случае -- гиперплоскости), разделюящие выборку. Справа показана прямая, максимизирующая отступ между классами. Источник: [лекция](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote09.html) Cornell\n",
    "```\n",
    "\n",
    "```{note}\n",
    "Одним из ключевых авторов алгоритма SVM является Владимир Вапник -- советский и американский (с 1991-го года) ученый, который также сделал огромный вклад в теорию классического машинного обучения. Его имя носит один из ключевых теоретических концептов машинного обучения -- размерность Вапника-Червоненкиса.\n",
    "```\n",
    "\n",
    "## Метод опорных векторов для линейно-разделимой выборки\n",
    "\n",
    "Будем рассматривать линейные классификаторы вида\n",
    "\n",
    "$$ a(x) = sign (\\langle w, x \\rangle + b), \\qquad w \\in \\mathbb{R}^d, b \\in \\mathbb{R}. $$\n",
    "\n",
    "Будем считать, что существуют такие параметры $w_*$ и $b_*$, что соответствующий им классификатор $a(x)$ не допускает ни одной ошибки на обучающей выборке. В этом случае говорят, что выборка _линейно разделима_.\n",
    "\n",
    "\n",
    "Заметим, что если одновременно умножить параметры $w$ и $b$ на одну и ту же положительную константу, то классификатор не изменится. Распорядимся этой свободой выбора и отнормируем параметры так, что\n",
    "\n",
    "$$\n",
    "    \\min_{x \\in X} | \\langle w, x \\rangle + b| = 1.\n",
    "$$\n",
    "\n",
    "Можно показать, что расстояние от произвольной точки $x_0 \\in \\mathbb{R}^d$ до гиперплоскости, определяемой данным классификатором, равно\n",
    "\n",
    "$$\n",
    "    \\rho(x_0, a)\n",
    "    =\n",
    "    \\frac{\n",
    "        |\\langle w, x \\rangle + b|\n",
    "    }{\n",
    "        \\|w\\|\n",
    "    }.\n",
    "$$\n",
    "\n",
    "Тогда расстояние от гиперплоскости до ближайшего примера из обучающей выборки равно\n",
    "\n",
    "$$\n",
    "    \\min_{x \\in X}\n",
    "    \\frac{\n",
    "        |\\langle w, x \\rangle + b|\n",
    "    }{\n",
    "        \\|w\\|\n",
    "    }\n",
    "    =\n",
    "    \\frac{1}{\\|w\\|} \\min_{x \\in X} |\\langle w, x \\rangle + b|\n",
    "    =\n",
    "    \\frac{1}{\\|w\\|}.\n",
    "$$\n",
    "\n",
    "Данная величина также называется __отступом__ (margin).\n",
    "\n",
    "Таким образом, если классификатор без ошибок разделяет обучающую выборку, то ширина его разделяющей полосы равна $\\frac{2}{\\|w\\|}$.\n",
    "Максимизация ширины разделяющей полосы приводит\n",
    "к повышению обобщающей способности классификатора {cite}`MohriRostamizadehTalwalkar18 `. На повышение обобщающей способности направлена и регуляризация,\n",
    "которая штрафует большую норму весов -- а чем больше норма весов,\n",
    "тем меньше ширина разделяющей полосы.\n",
    "\n",
    "Итак, требуется построить классификатор, идеально разделяющий обучающую выборку, и при этом имеющий максимальный отступ.\n",
    "\n",
    "Запишем соответствующую оптимизационную задачу,\n",
    "которая и будет определять метод опорных векторов для линейно разделимой выборки (hard margin support vector machine):\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\label{eq:svmSep}\n",
    "    \\left\\{\n",
    "        \\begin{aligned}\n",
    "            & \\frac{1}{2} \\|w\\|^2 \\to \\min_{w, b} \\\\\n",
    "            & y_i \\left(\n",
    "                \\langle w, x_i \\rangle + b\n",
    "            \\right) \\geq 1, \\quad i = 1, \\dots, \\ell.\n",
    "        \\end{aligned}\n",
    "    \\right.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Здесь мы воспользовались тем, что линейный классификатор дает правильный ответ на примере $x_i$ тогда и только тогда, когда $y_i (\\langle w, x_i \\rangle + b) \\geq 0$ (величина $M_i = y_i (\\langle w, x_i \\rangle + b)$ также называется __отступом примера__ $(x_i, y_i)$ обучающей выборки). Более того, из условия нормировки $\\min_{x \\in X} | \\langle w, x \\rangle + b| = 1$ следует, что $y_i (\\langle w, x_i \\rangle + b) \\geq 1$.\n",
    "\n",
    "В данной задаче функционал является строго выпуклым, а ограничения линейными, поэтому сама задача является выпуклой и имеет единственное решение. Более того, задача является квадратичной и может быть решена крайне эффективно.\n",
    "\n",
    "## Метод опорных векторов для линейно-неразделимой выборки\n",
    "\n",
    "Рассмотрим теперь общий случай, когда выборку\n",
    "невозможно идеально разделить гиперплоскостью.\n",
    "Это означает, что какие бы $w$ и $b$ мы ни взяли,\n",
    "хотя бы одно из ограничений в предыдущей задаче\n",
    "будет нарушено:\n",
    "\n",
    "$$\n",
    "    \\exists \\ x_i \\in X:\\\n",
    "    y_i \\left(\n",
    "        \\langle w, x_i \\rangle + b\n",
    "    \\right) < 1.\n",
    "$$\n",
    "\n",
    "Сделаем эти ограничения _мягкими_, введя штраф $\\xi_i \\geq 0$ за их нарушение:\n",
    "\n",
    "$$\n",
    "    y_i \\left(\n",
    "        \\langle w, x_i \\rangle + b\n",
    "    \\right) \\geq 1 - \\xi_i, \\quad i = 1, \\dots, \\ell.\n",
    "$$\n",
    "\n",
    "Отметим, что если отступ объекта лежит между нулем и\n",
    "единицей ($0 \\leq y_i \\left( \\langle w, x_i \\rangle + b \\right) < 1$), то объект верно классифицируется, но имеет ненулевой штраф $\\xi > 0$. Таким образом, мы штрафуем объекты за попадание внутрь разделяющей полосы.\n",
    "\n",
    "Величина $\\frac{1}{\\|w\\|}$ в данном случае называется __мягким отступом__ (soft margin). С одной стороны, мы хотим максимизировать отступ, с другой -- минимизировать\n",
    "штраф за неидеальное разделение выборки $\\sum_{i = 1}^{\\ell} \\xi_i$.\n",
    "\n",
    "Эти две задачи противоречат друг другу: как правило, излишняя подгонка под выборку приводит к маленькому отступу, и наоборот -- максимизация отступа приводит к большой ошибке на обучении.\n",
    "В качестве компромисса будем минимизировать взвешенную сумму двух указанных величин.\n",
    "\n",
    "Приходим к оптимизационной задаче, соответствующей методу опорных векторов для линейно неразделимой выборки (soft margin support vector machine):\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\label{eq:svmUnsep}\n",
    "    \\left\\{\n",
    "        \\begin{aligned}\n",
    "            & \\frac{1}{2} \\|w\\|^2 + C \\sum_{i = 1}^{\\ell} \\xi_i \\to \\min_{w, b, \\xi} \\\\\n",
    "            & y_i \\left(\n",
    "                \\langle w, x_i \\rangle + b\n",
    "            \\right) \\geq 1 - \\xi_i, \\quad i = 1, \\dots, \\ell, \\\\\n",
    "            & \\xi_i \\geq 0, \\quad i = 1, \\dots, \\ell.\n",
    "        \\end{aligned}\n",
    "    \\right.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Чем больше здесь параметр $C$, тем сильнее мы будем настраиваться на обучающую выборку. Данная задача также является выпуклой и имеет единственное решение.\n",
    "\n",
    "### Сведение к безусловной задаче оптимизации\n",
    "\n",
    "Покажем, что задачу метода опорных векторов можно свести к задаче безусловной оптимизации функционала, который имеет вид верхней оценки на долю неправильных ответов.\n",
    "\n",
    "Перепишем условия задачи:\n",
    "\n",
    "$$\n",
    "    \\left\\{\n",
    "    \\begin{aligned}\n",
    "        &\\xi_i \\geq 1 - y_i (\\langle w, x_i \\rangle + b) \\\\\n",
    "        &\\xi_i \\geq 0\n",
    "    \\end{aligned}\n",
    "    \\right.\n",
    "$$\n",
    "\n",
    "Поскольку при этом в функционале требуется, чтобы штрафы $\\xi_i$ были как можно меньше, то можно получить следующую явную формулу для них:\n",
    "\n",
    "$$\n",
    "    \\xi_i\n",
    "    =\n",
    "    \\max(0,\n",
    "        1 - y_i (\\langle w, x_i \\rangle + b)).\n",
    "$$\n",
    "\n",
    "Данное выражение для $\\xi_i$ уже учитывает в себе все ограничения задачи, описанной выше.\n",
    "Значит, если подставить его в функционал, то получим безусловную задачу оптимизации:\n",
    "\n",
    "$$\n",
    "    \\frac{1}{2} \\|w\\|^2\n",
    "    +\n",
    "    C\n",
    "    \\sum_{i = 1}^{\\ell}\n",
    "        \\max(0,\n",
    "            1 - y_i (\\langle w, x_i \\rangle + b))\n",
    "    \\to\n",
    "    \\min_{w, b}\n",
    "$$\n",
    "\n",
    "Эта задача является негладкой, поэтому решать её может быть достаточно тяжело. Тем не менее, она показывает, что метод опорных векторов, по сути, как и логистическая регрессия, строит верхнюю оценку на долю ошибок и добавляет к ней стандартную квадратичную регуляризацию. Только если в случае логистической регрессии этой верхней оценкой была логистическая функция потерь, то в случае метода опорных векторов это функция вида  $L(y, z) = \\max(0, 1 - yz)$, которая называется __кусочно-линейной функцией потерь (hinge loss)__.\n",
    "\n",
    "```{figure} /_static/qsvmblock/classicsvm/loss_functions_logistic_and_hinge.png\n",
    ":width: 450px\n",
    ":name: loss_functions_logistic_and_hinge\n",
    "\n",
    "Пороговая, кусочно-линейная и логистическая функции потерь\n",
    "```\n",
    "\n",
    "Это становится понятнее в терминах упомянутого выше отступа $M_i = y_i (\\langle w, x_i \\rangle + b)$ на примере обучающей выборки. В идеале мы хотели бы штрафовать классификатор за ошибку на примере: $L_{1/0} (M_i) = [M_i < 0]$. Это пороговая функция потерь (zero-one loss), ее график изображен черным на рисунке  {numref}`loss_functions_logistic_and_hinge` как функция от отступа. К сожалению, напрямую мы не можем эффективно оптимизировать такую функцию, поэтому оптимизируется ее верхняя оценка. В случае логистической регрессии – логистическая функция потерь $L(M_i) = \\log(1+ e^{-M_i})$ (красная на рисунке выше), а в случае метода опорных векторов -- кусочно-линейная функция $L(M_i) = \\max(0, 1 - M_i)$ (зеленая на рисунке выше).\n",
    "\n",
    "## Решение задачи метода опорных векторов и ядровый переход\n",
    "\n",
    "Итак, метод опорных векторов сводится к решению задачи оптимизации\n",
    "\n",
    "$$\n",
    "    \\left\\{\n",
    "        \\begin{aligned}\n",
    "            & \\frac{1}{2} \\|w\\|^2 + C \\sum_{i = 1}^{\\ell} \\xi_i \\to \\min_{w, b, \\xi} \\\\\n",
    "            & y_i \\left(\n",
    "                \\langle w, x_i \\rangle + b\n",
    "            \\right) \\geq 1 - \\xi_i, \\quad i = 1, \\dots, \\ell, \\\\\n",
    "            & \\xi_i \\geq 0, \\quad i = 1, \\dots, \\ell.\n",
    "        \\end{aligned}\n",
    "    \\right.\n",
    "$$\n",
    "\n",
    "Для решения таких _условных_ задач оптимизации с условиями в виде неравенств или равенств часто используют лагранжиан и двойственную задачу оптимизации. Этот подход исчерпывающе описан в классической книге Бойда по оптимизации {cite}`Boyd2006`, а на русском языке можно обратиться к [конспекту](http://www.machinelearning.ru/wiki/images/f/fe/Sem6_linear.pdf) Евгения Соколова. Также для понимания материала рекомендуется рассмотреть [\"игрушечный\" пример](https://github.com/esokolov/ml-course-hse/blob/master/2016-spring/seminars/sem16-svm.pdf) решения задачи метода опорных векторов в случае линейно-разделимой выборки из 5 примеров.\n",
    "\n",
    "Построим двойственную задачу к задаче метода опорных векторов.\n",
    "Запишем лагранжиан:\n",
    "\n",
    "$$\n",
    "    L(w, b, \\xi, \\lambda, \\mu)\n",
    "    =\n",
    "    \\frac{1}{2} \\|w\\|^2 + C \\sum_{i = 1}^{\\ell} \\xi_i\n",
    "    -\n",
    "    \\sum_{i = 1}^{\\ell} \\lambda_i \\left[\n",
    "        y_i \\left(\n",
    "                \\langle w, x_i \\rangle + b\n",
    "            \\right) - 1 + \\xi_i\n",
    "    \\right]\n",
    "    -\n",
    "    \\sum_{i = 1}^{\\ell}\n",
    "        \\mu_i \\xi_i.\n",
    "$$\n",
    "\n",
    "Выпишем условия Куна-Таккера:\n",
    "\n",
    "```{math}\n",
    ":label: KuckTuckerCond1\n",
    "\n",
    "\\nabla_w L = w - \\sum_{i = 1}^{\\ell} \\lambda_i y_i x_i = 0\n",
    "  \\quad\\Longrightarrow\\quad\n",
    "    w = \\sum_{i = 1}^{\\ell} \\lambda_i y_i x_i\n",
    "```\n",
    "\n",
    "```{math}\n",
    ":label: KuckTuckerCond2\n",
    "\n",
    "\\nabla_b L = - \\sum_{i = 1}^{\\ell} \\lambda_i y_i = 0\n",
    "    \\quad\\Longrightarrow\\quad\n",
    "    \\sum_{i = 1}^{\\ell} \\lambda_i y_i = 0\n",
    "```\n",
    "\n",
    "```{math}\n",
    ":label: KuckTuckerCond3\n",
    "\n",
    "  \\nabla_{\\xi_i} L = C - \\lambda_i - \\mu_i\n",
    "    \\quad\\Longrightarrow\\quad\n",
    "    \\lambda_i + \\mu_i = C\n",
    "```\n",
    "\n",
    "```{math}\n",
    ":label: KuckTuckerCond4\n",
    "\n",
    "\\lambda_i \\left[\n",
    "        y_i \\left(\n",
    "                \\langle w, x_i \\rangle + b\n",
    "            \\right) - 1 + \\xi_i\n",
    "        \\right] = 0\n",
    "    \\quad\\Longrightarrow\\quad\n",
    "    (\\lambda_i = 0)\n",
    "        \\ \\text{или}\\\n",
    "        \\left(\n",
    "            y_i \\left(\n",
    "                \\langle w, x_i \\rangle + b\n",
    "            \\right)\n",
    "            =\n",
    "            1 - \\xi_i\n",
    "        \\right)\n",
    "```\n",
    "\n",
    "\n",
    "```{math}\n",
    ":label: KuckTuckerCond5\n",
    "\n",
    "\\mu_i \\xi_i = 0\n",
    "    \\quad\\Longrightarrow\\quad\n",
    "    (\\mu_i = 0)\n",
    "        \\ \\text{или}\\\n",
    "        (\\xi_i = 0) \\\\\n",
    "    %\n",
    "    \\xi_i \\geq 0, \\lambda_i \\geq 0, \\mu_i \\geq 0.\n",
    "```\n",
    "\n",
    "Проанализируем полученные условия.\n",
    "\n",
    "Из первого следует, что вектор весов, полученный в результате настройки SVM, можно записать как линейную комбинацию объектов, причем веса в этой линейной комбинации можно найти как решение двойственной задачи.\n",
    "\n",
    "В зависимости от значений $\\xi_i$ и $\\lambda_i$ объекты $x_i$ разбиваются на три категории:\n",
    "\n",
    "- $\\xi_i = 0$, $\\lambda_i = 0$. Такие объекты не влияют решение $w$ (входят в него с нулевым весом $\\lambda_i$),\n",
    "        правильно классифицируются~($\\xi_i = 0$) и лежат вне разделяющей полосы.\n",
    "        Объекты этой категории называются _периферийными_.\n",
    "- $\\xi_i = 0$, $0 < \\lambda_i < C$. Из условия \\eqref{eq:svmKktSlack1} следует, что $y_i \\left(\\langle w, x_i \\rangle + b \\right) = 1$,\n",
    "        то есть объект лежит строго на границе разделяющей полосы.\n",
    "        Поскольку $\\lambda_i > 0$, объект влияет на решение $w$.\n",
    "        Объекты этой категории называются _опорными граничными_.\n",
    "- $\\xi_i > 0$, $\\lambda_i = C$. Такие объекты могут лежать внутри разделяющей полосы~($0 < \\xi_i < 2$) или выходить за ее пределы~($\\xi_i \\geq 2$). При этом если $0 < \\xi_i < 1$, то объект классифицируется правильно, в противном случае -- неправильно. Объекты этой категории называются _опорными нарушителями_.\n",
    "\n",
    "Отметим, что варианта $\\xi_i > 0$, $\\lambda_i < C$ быть не может,\n",
    "поскольку при $\\xi_i > 0$ из условия дополняющей нежесткости~\\eqref{eq:svmKktSlack2}\n",
    "следует, что $\\mu_i = 0$, и отсюда из уравнения~\\eqref{eq:svmKktXi} получаем,\n",
    "что $\\lambda_i = C$.\n",
    "\n",
    "Итак, итоговый классификатор зависит только от объектов, лежащих\n",
    "на границе разделяющей полосы, и от объектов-нарушителей (с $\\xi_i > 0$).\n",
    "\n",
    "Построим двойственную функцию. Для этого подставим выражение~\\eqref{eq:svmKktW} в лагранжиан,\n",
    "и воспользуемся уравнениями~\\eqref{eq:svmKktB} и~\\eqref{eq:svmKktXi}~(данные\n",
    "три уравнения выполнены для точки минимума лагранжиана при\n",
    "любых фиксированных $\\lambda$ и $\\mu$):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    L &= \\frac{1}{2} \\left\\|\n",
    "            \\sum_{i = 1}^{\\ell}\n",
    "                \\lambda_i y_i x_i\n",
    "        \\right\\|^2\n",
    "        -\n",
    "        \\sum_{i, j = 1}^{\\ell}\n",
    "            \\lambda_i \\lambda_j y_i y_j \\langle x_i, x_j \\rangle\n",
    "        -\n",
    "        b\n",
    "        \\underbrace{\\sum_{i = 1}^{\\ell}\n",
    "            \\lambda_i y_i}_{0}\n",
    "        +\n",
    "        \\sum_{i = 1}^{\\ell}\n",
    "            \\lambda_i\n",
    "        +\n",
    "        \\sum_{i = 1}^{\\ell}\n",
    "            \\xi_i \\underbrace{(C - \\lambda_i - \\mu_i)}_{0} \\\\\n",
    "    &=\n",
    "    \\sum_{i = 1}^{\\ell}\n",
    "        \\lambda_i\n",
    "    -\n",
    "    \\frac{1}{2} \\sum_{i, j = 1}^{\\ell}\n",
    "        \\lambda_i \\lambda_j y_i y_j \\langle x_i, x_j \\rangle.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Мы должны потребовать выполнения условий~\\eqref{eq:svmKktB}\n",
    "и~\\eqref{eq:svmKktXi}~(если они не выполнены,\n",
    "то двойственная функция обращается в минус бесконечность),\n",
    "а также неотрицательность двойственных\n",
    "переменных $\\lambda_i \\geq 0$, $\\mu_i \\geq 0$.\n",
    "Ограничение на $\\mu_i$ и условие~\\eqref{eq:svmKktXi},\n",
    "можно объединить, получив $\\lambda_i \\leq C$.\n",
    "Приходим к следующей двойственной задаче:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\label{eq:svmDual}\n",
    "    \\left\\{\n",
    "        \\begin{aligned}\n",
    "            & \\sum_{i = 1}^{\\ell}\n",
    "                \\lambda_i\n",
    "            -\n",
    "            \\frac{1}{2} \\sum_{i, j = 1}^{\\ell}\n",
    "                \\lambda_i \\lambda_j y_i y_j \\langle x_i, x_j \\rangle\n",
    "            \\to \\max_{\\lambda} \\\\\n",
    "            & 0 \\leq \\lambda_i \\leq C, \\quad i = 1, \\dots, \\ell, \\\\\n",
    "            & \\sum_{i = 1}^{\\ell} \\lambda_i y_i = 0.\n",
    "        \\end{aligned}\n",
    "    \\right.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Она также является вогнутой, квадратичной и имеет единственный максимум.\n",
    "\n",
    "Двойственная задача SVM зависит только от скалярных произведений объектов~---\n",
    "отдельные признаковые описания никак не входят в неё.\n",
    "Значит, можно легко сделать ядровой переход:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\left\\{\n",
    "        \\begin{aligned}\n",
    "            & \\sum_{i = 1}^{\\ell}\n",
    "                \\lambda_i\n",
    "            -\n",
    "            \\frac{1}{2} \\sum_{i, j = 1}^{\\ell}\n",
    "                \\lambda_i \\lambda_j y_i y_j K(x_i, x_j)\n",
    "            \\to \\max_{\\lambda} \\\\\n",
    "            & 0 \\leq \\lambda_i \\leq C, \\quad i = 1, \\dots, \\ell, \\\\\n",
    "            & \\sum_{i = 1}^{\\ell} \\lambda_i y_i = 0.\n",
    "        \\end{aligned}\n",
    "    \\right.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Вернемся к тому, какое представление классификатора дает двойственная задача.\n",
    "Из уравнения \\eqref{eq:svmKktW} следует, что вектор весов $w$\n",
    "можно представить как линейную комбинацию объектов из обучающей выборки.\n",
    "Подставляя это представление $w$ в классификатор, получаем\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\label{eq:svmDualClassifier}\n",
    "    a(x) = sign \\left(\n",
    "        \\sum_{i = 1}^{\\ell} \\lambda_i y_i \\langle x_i, x \\rangle + b\n",
    "    \\right).\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Таким образом, классификатор измеряет сходство нового объекта\n",
    "с объектами из обучения, вычисляя скалярное произведение между ними.\n",
    "Это выражение также зависит только от скалярных произведений,\n",
    "поэтому в нём тоже можно перейти к ядру.\n",
    "\n",
    "В представлении фигурирует переменная $b$,\n",
    "которая не находится непосредственно в двойственной задаче.\n",
    "Однако ее легко восстановить по любому граничному опорному объекту $x_i$, для которого выполнено $\\xi_i = 0, 0 < \\lambda_i < C$.\n",
    "Для него выполнено $y_i \\left(\\langle w, x_i \\rangle + b \\right) = 1$, откуда получаем\n",
    "\n",
    "$$\n",
    "    b = y_i - \\langle w, x_i \\rangle.\n",
    "$$\n",
    "\n",
    "Как правило, для численной устойчивости берут медиану данной величины по\n",
    "всем граничным опорным объектам:\n",
    "\n",
    "$$\n",
    "    b = med \\ y_i - \\langle w, x_i \\rangle, \\xi_i = 0, 0 < \\lambda_i < C.\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "source_map": [
   11
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}