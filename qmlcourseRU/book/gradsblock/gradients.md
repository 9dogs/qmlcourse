---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

(gradients)=

# Градиенты квантовых схем

## Описание лекции

В этой лекции мы детально разберем, как можно оптимизировать параметры VQC:

- Как выглядит цикл обучения квантовой схемы
- Как работает оценка градиента "под капотом"
  - Метод конечных отрезком
  - Parameter-shift rule

## Введение

Как мы уже говорили ранее, VQC выступают в роли "черных ящиков", которые имеют параметры и как-то преобразуют поступающие в них данные. В этом случае, сам процесс оптимизации параметров выполняется на классическом компьютере. Одними из самых эффективных на сегодня методов решения задач непрерывной оптимизации являются градиентные методы. Для этих методов разработан широкий арсенал эвристик и приемов, который применяется в обучении классических глубоких нейронных сетей. Очень хочется применить весь этот арсенал и для квантового машинного обучения. Но как же посчитать градиент вариационной квантовой схемы?

## Задача лекции

На этой лекции мы рассмотрим простую задачку по оптимизации параметров квантовой схемы и на ее примере увидим, как работают квантовые градиенты. В качестве задачи возьмем известный набор данных "Two Moon" из библиотеки `scikit-learn`:

```{code-cell} ipython3
from pennylane import numpy as np
import pennylane as qml
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons

x, y = make_moons(n_samples=50)
```

Для удобства мы сразу переведем метки классов из $\{0, 1\} \to \{-1, 1\}$ и посмотрим, как выглядит наш набор данных:

```{code-cell} ipython3
y = y * 2 - 1

plt.figure(figsize=(8, 7))
cb = plt.scatter(x[:, 0], x[:, 1], c=y)
plt.colorbar(cb)
plt.show()
```

## Вариационнная схема

Перед тем, как мы начнем обучение и будем считать градиенты нам необходимо определиться с тем, как будет выглядеть наша вариационная схема. Мы посвятим кодированию данных, выбору архитектуры схемы, а также измеряемого оператора еще много занятий. Так что пока просто воспользуемся кодирование признаков $\mathbf{X}$ в амплитуды состояний, а сверху применим несколько параметризованных слоев вращений.

### Кодирование признаков в амплитуды

```{code-cell} ipython3
dev = qml.device("default.qubit", 2)

def encoding(x1, x2):
    qml.templates.AmplitudeEmbedding([x1, x2], wires=[0, 1], pad=0.0, normalize=True)
```

### Параметризованные слои

В качестве одного слоя обучения мы будем использовать параметризованные вращения в связке с двухкубитным гейтом для создания запутанных состояний.

```{note}
Более детально о запутанных состояниях, а также квантовой энтропии и черных дырах можно посмотреть в продвинутой лекции блока про квантовые вычисления.
```

В этой лекции у нас нет цели идеально решить поставленную задачу -- на самом деле это чуть сложнее, чем может показаться на первый взгляд. Поэтому пока не будем излишне усложнять нашу **VQC**.

```{code-cell} ipython3
def layer(q):
    qml.Rot(q[0, 0], q[0, 1], q[0, 2], wires=0)
    qml.Rot(q[1, 0], q[1, 1], q[1, 2], wires=1)
    qml.CZ(wires=[0, 1])
```

Здесь у нас вращения каждого из кубитов по сфере Блоха и двухкубитное взаимодействие $\hat{CZ}$.

### Все вместе

Теперь давайте объединим все это вместе, добавим пару наблюдаемых и оформим как `qml.qnode`:

```{code-cell} ipython3
@qml.qnode(dev)
def node(x1, x2, q):
    encoding(x1, x2)
    for q_ in q:
        layer(q_)

    return qml.expval(qml.PauliZ(0) @ qml.PauliY(1))
```

### Функция "скоринга"

Наша квантовая схема принимает на вход лишь одну точку данных, а у нас их 50. Поэтому удобно сразу написать функцию, которая может работать с массивами `numpy`:

```{code-cell}
def apply_node(x, q):
    return [node(x_[0], x_[1], q[0]) + q[1] for x_ in x]
```

### Визуализация

Давайте инициализируем нашу схему случайными параметрами и посмотрим, как она "сходу" классифицирует данные. Возьмем 4 параметризованных слоя.

```{code-cell} ipython3
np.random.seed(42)
q = (np.random.uniform(-np.pi, np.pi, size=(4, 2, 3)), 0.0)

plt.figure(figsize=(8, 7))
cb = plt.scatter(x[:, 0], x[:, 1], c=apply_node(x, q))
plt.colorbar(cb)
plt.show()
```

Как видно, результат "не очень" и наша цель попытаться его улучшить.

## Функция потерь

Прежде чем варьировать параметры схемы, нам для начала необходимо понять, а что именно мы хотим оптимизировать? Для этого нам необходимо выбрать функцию потерь.

```{note}
Если у Вас трудности с функциями потерь в таком контексте, то рекомендуем вернуться к вводной лекции про классическое машинное обучения, где это тема раскрыта с нуля и достаточно подробно.
```

### Квадратичное отклонение

В качестве функции потерь, которая является дифференцируемой, мы будем использовать наиболее простой вариант -- среднеквадратичное отклонения. Это не самый лучший выбор для задач классификации, но зато самый простой. Простой вариант -- это именно то, что нам нужно в этой лекции:

```{code-cell} ipython3
def cost(q, x, y):
    preds = np.array(apply_node(x, q))
    return np.mean(np.square(preds - y))
```

### Точность классификации

В качестве метрики качества среднеквадратичное отклонение вообще не подходит -- понять по этой цифре, хорошо или плохо работает модель почти невозможно! Поэтому для оценки модели в целом мы будем использовать точность:

```{code-cell} ipython3
def acc(q, x, y):
    preds = np.sign(apply_node(x, q))
    res = 0
    for p_, y_ in zip(preds, y):
        if np.abs(y_ - p_) <= 1e-2:
            res += 1

    return res / y.shape[0]
```

## Решение средствами `PennyLane`

Библиотека `PennyLane` может использовать один из нескольких движков для автоматического дифференцирования:

- `NumPy Autograd`
- `PyTorch`
- `Tensorflow`
- `Jax`

По больше части, на наших занятиях мы будем использовать `NumPy` из-за простоты и привычности. Перед тем, как разбираться с тем, как же именно происходит дифференцирование квантовой схемы, давайте посмотрим на весь цикл обучения.

```{code-cell} ipython3
opt = qml.optimize.GradientDescentOptimizer(stepsize=0.05)
acc_ = []
cost_ = []
ii = []
for i in range(30):
    batch = np.random.randint(0, len(x), (25,))
    x_batch = x[batch, :]
    y_batch = y[batch]
    q = opt.step(lambda q_: cost(q_, x_batch, y_batch), q)

    if i % 5 == 0:
        ii.append(i)
        acc_.append(acc(q, x, y))
        cost_.append(cost(q, x, y))
```

И посмотрим на получившиеся графики точности и функции потерь:

```{code-cell} ipython3
f, ax = plt.subplots(2, figsize=(8, 6), sharex=True)
ax[0].plot(ii, acc_, ".-")
ax[0].set_title("Accuracy")
ax[1].plot(ii, cost_, ".-")
ax[1].set_title("Cost")
plt.show()
```

А также на результаты классификации:

```{code-cell} ipython3
plt.figure(figsize=(8, 7))
cb = plt.scatter(x[:, 0], x[:, 1], c=apply_node(x, q))
plt.colorbar(cb)
plt.show()
```
