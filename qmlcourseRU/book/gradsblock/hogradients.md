---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

(hogradients)=

# Градиенты высших порядков

## План лекции

В этой лекции мы посмотрим на ту математику, которая лежит "под капотом" у _parameter-shift rule_. Также мы увидим, как можно оптимизировать этот метод, а еще узнаем как можно посчитать производную второго порядка. Дополнительно мы коснемся темы различных оптимизаторов применительно вариационных квантовых схем.

Для более детального погружения в вопрос можно сразу рекомендовать статью {cite}`hogradients`.

## Важность гейтов вращений

Если задуматься, то одним из основных (если не единственных) способов сделать параметризованную квантовую схему является использование гейтов вращений, таких как $\hat{RX}, \hat{RY}, \hat{RZ}$. Более формально это можно выразить так, что нас больше всего интересуют операторы вида:

$$
U(\theta) = e^{-\frac{i}{2}H\theta}
$$

где $H$ -- матрица такая, что $H^2 = \mathbf{1}$. Другой возможный вариант записи -- представить матрицу $H$ как линейную комбинацию операторов Паули $\sigma^x, \sigma^y, \sigma^z$.

Если представить схему, содержащую множество параметризованных операторов, то итоговая запись имеет вид:

$$
U_{j...k} = U_j, ..., U_k \ket{\Psi}
$$

## Производная от измерения

Давайте вспомним, как выглядит квантово-классическая схема обучения с **VQC**.

```{figure} /_static/vqcblock/vqc/diagram.png
:name: quantclassical
:height: 400px

Квантово-классическая схема
```

Видно, что мы хотим считать производную не от самой параметризованной схемы $U_{j...k}$, а от наблюдаемой. Для тех, кто забыл, что такое _наблюдаемая_, рекомендуем вернуться к лекции про кубит. Если кратко, то это тот оператор, который мы "измеряем" на нашем квантовом компьютере. Математически производная, которая нам интересна может быть записана для выбранного параметра $i$ таким образом:

$$
G_i = \frac{\partial \bra{U_{j...k}\Psi}\hat{M}\ket{U_{j...k}\Psi}}{\partial \theta_i}
$$

То есть нам важно посчитать производную от результата измерения, так как именно результат измерения у нас будет определять "предсказание" нашей квантовой нейронной сети. Причем нам нужно уметь считать производную от любого параметра $\theta_i$ в цепочке $\theta_j, ...\theta_i, ...\theta_k$.

## Parameter-shift для гейтов Паули

Так как матрица $H$ может быть декомпозирована на гейты Паули, то сам оператор $U_i$ может быть также записан так:

$$
U_i = e^{-\frac{i}{2}P_i\theta_i}
$$

```{note}
Тут мы для простоты предложим, что $U_1$ это просто оператор вращения, иначе выкладки станут совсем сложными.
```

Запишем результат математического ожидания через состояние $\Psi_i$, которое пришло на вход $i$-го гейта в нашей последовательности:

$$
\langle M(\theta) \rangle = Tr(M U_{k, ..., 1} \Psi_i U_{k, ..., 1}^\dagger)
$$

Тогда частная производная от математического ожидания по $i$-му параметру $\theta_i$ записывается (подробнее в {cite}`parametershift`) через коммутатор исходного состояния $\ket{\Psi_i}$, которое "пришло" на вход гейта $U_i$ и того оператора Паули $P_i$, который мы используем в $U_i$:

$$
\frac{\partial \langle M \rangle}{\partial \theta_i} = -\frac{i}{2}Tr(M U_{k, ..., i}[P_i, U_{i-1, ..., 1}\Psi_i U_{i-1, ..., 1}^\dagger]U_{k, ..., i}^\dagger)
$$

Этот коммутатор может быть переписан следующим образом:

$$
[P_i, \Psi] = i[U_i \Bigl( \frac{\pi}{2} \Bigr ) \Psi_i U_i^\dagger \Bigl ( \frac{\pi}{2} \Bigr ) - U_i \Bigl( -\frac{\pi}{2} \Bigr ) \Psi_i U_i^\dagger \Bigl ( -\frac{\pi}{2} \Bigr )]
$$

Тогда соответствующий градиент $\frac{\partial}{\partial \theta_i}$ можно записать через смещения на $\pm\frac{\pi}{2}$:

$$
\begin{gathered}
\frac{\partial \langle M \rangle}{\partial \theta_i} = \frac{\langle M_i^+ \rangle - \langle M_i^- \rangle}{2} \\
M_i^{\pm} = \frac{1}{2} Tr [M U_{k, ..., i+1} U_i(\pm \frac{\pi}{2})\Psi_i^` U_i^\dagger (\pm \frac{\pi}{2}) U_{k, ..., i+1}^\dagger] \\
\Psi_i^` = U_{j, ..., 1} \Psi_i U_{j, ..., 1}^\dagger
\end{gathered}
$$

По аналогии с классическими нейронными сетями и _backpropagation_ (для тех, кто забыл это понятие, рекомендуем вернуться к вводным лекциями про классическое машинное обучение) тут явно можно выделить _forward_ проход со смещением $\theta_i$ на значения $\frac{\pi}{2}$ и _backward_ со смещением на $-\frac{\pi}{2}$.

## Обобщенный parameter-shift

Предложенное в {cite}`parametershift` выражение может быть на самом деле получено в более общем виде из других соображений. Так, выражение для нашей наблюдаемой $\langle M \rangle$ может всегда быть представлено {cite}`hogradients` как сумма вида:

$$
\bra{U_i(\theta_i)\Psi}\hat{M}\ket{U_i(\theta_i)\Psi} = \hat{A} + \hat{B}\cos{\theta_i} + \hat{C}\sin{\theta_i}
$$

где $\hat{A}, \hat{B}, \hat{C}$ -- операторы, не зависящие от параметра $\theta_i$.

Тогда, можно воспользоваться правилами тригонометрии, а именно тем, что для любого $s \neq k\pi, \text{   } k \in {1, 2, ..., }$ справедливо:

$$
\begin{gathered}
\frac{d \cos {\theta}}{d \theta} = \frac{\cos (\theta + s) - \cos (\theta - s)}{2\sin{s}} \\
\frac{d \sin {\theta}}{d \theta} = \frac{\sin (\theta + s) - \sin (\theta - s)}{2\sin{s}} \\
\end{gathered}
$$

И подставим это в выражение для $\frac{\partial \langle M \rangle}{\partial \theta_i}$:

$$
\frac{\partial \langle M(\theta_i) \rangle}{\partial \theta_i} = \frac{\langle M(\theta_i + s) \rangle - \langle M(\theta_i - s) \rangle}{2\sin{s}}
$$

Легко заметить, что подстановка сюда $s = \frac{\pi}{2}$ дает нам классический _parameter shift_, описанный в {cite}`parametershift`.
