---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

(qknn)=

# Quantum K-nearest neighbor 

Если вы занимались машинным обучением, то, скорее всего, знакомы с алгоритмом $k$ ближайших соседей. Он относительно прост, применяется как в задачах классификации, так и в регрессии.
Кстати, классический knn можно вспомнить обратившиь к лекциям от ODS по классическому машинному обучению.

Давайте немножко вспомним задачу классификации с использованием классического $kNN$ алгоритма:

У нас есть $\vec{x} \in \{0, 1 \}^N$ - **тестовый образец**, а также **тренировочные образцы** - это набор векторов $\vec{v_i} \in \{0, 1 \}^N$, в котором каждый вектор уже размечен. И наша задача подобрать правильную метку тестовому образцу.

Тогда мы пройдём следующие шаги:

1. Вычислим похожесть между тестовым образцом и **каждым** тренировочным образцом.
2. Найдем $k$ ближайших к тестовому образцу соседей.
3. Подсчитаем количество представителей для каждого класса и приписываем метку самого часто встречающегося класса к тестовому образцу.

Самой трудозатратной является вычисление расстояния от тестового образца к каждому тренировочному.


```{note}
[Расстояние Хэмминга](https://ru.wikipedia.org/wiki/Расстояние_Хэмминга) между векторами $\vec{x}$ и $\vec{v_i}$:

$$ d_i = |\vec{x} - \vec{v_i}| = \sum_{j=1}^{N} (x_j \oplus v_{ij}) $$
```

Получается, что нам обязательно нужен квантовый алгоритм для подсчёта Расстояния Хэмминга. И такой мы сейчас разберём.

