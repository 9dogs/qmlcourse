---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

(vqe)=

# Variational Quantum Eigensolver

В этой лекции мы рассмотрим такой классный и очень перспективный алгоритм как `Variational Quantum Eigensolver`. Это алгоритм, который создан для аппроксимационного решения задачи о собственных значениях матриц. Но, как мы увидим, к этой задаче можно свети большое число реально интересных задач, например, _NP_-трудные задачи комбинаторной оптимизации, о которых мы уже говорили в [лекции о квантовых алгоритмах](quantum_algorithms_overview).

Лекция будет построена следующим образом:

- Кратко вспомним задачу о собственных значениях
- Посмотрим, как эту задачу можно свести к градиентному спуску в пространстве параметров [параметризированной квантовой схемы](vqc)
- Поговорим о том, что такое гамильтониан Изинга
- Увидим, как задачи комбинаторной оптимизации сводятся к проблеме Изинга
- Полностью реализуем `VQE` и решим маленькую задачку Коммивояжера

## Задача о собственных значениях

Тем, кому этот блок сразу покажется трудным мы рекомендуем вернуться к вводным лекциям по линейной алгебре, а именно к [части про собственные вектора и собственные значения]().

Итак, пусть у нас есть матрица $A$ размерности $n \times n$, она же является линейным оператором $\hat{A}$. Из линейной алгебры мы знаем, что у этой матрицы есть $n$ таких чисел $e_i$ и таких векторов $\Psi_i$, что для них выполняется условие:

$$
A \Psi_i = e_i \Psi_i
$$

или в нотации Дирака, которая используется в области квантовых вычисления:

$$
\hat{A} \ket{\Psi_i} = e_i\ket{\Psi_i}
$$

Таким образом, собственные вектора это такие вектора, которые при применении оператора не меняют свое направление. Например, если $A$ это, например, оператор поворота, то его собственные вектора, это его оси симметрии:

```{figure} /_static/vqeblock/vqe/Mona_Lisa.png
:width: 450px
:name: Mona_Lisa_Eigen

Синий вектор, в отличии от красного, при применении матрицы поворота не меняет направление так как является собственным вектором этой матрицы.
```

### Алгоритмы нахождения собственных значений

В целом, задача нахождения собственных значений является очень трудной с вычислительной точки зрения, особенно для больших матриц. Для матриц размера более, чем $3 \times 3$ в общем случае не существует алгоритма нахождения собственных значений и собственных векторов. Однако существует несколько итеративных алгоритмов. Мы рассмотрим лишь два из них, причем без особых деталей, так как это эти алгоритмы, а также доказательство их сходимости являются достаточно сложными.

### Степенной метод

Один из самых простых для понимания алгоритмов, который, тем не менее находит интересные применения. Суть его в том, что мы берем некоторый случайный вектор $\ket{\Psi}$ и начинаем последовательно действовать на него оператором $\hat{A}$ (другими словами умножать на нашу матрицу), при этом нормируя на норму:

$$
\ket{\Psi_{i+1}} = \frac{\hat{A}\ket{\Psi_i}}{||\hat{A}||}
$$

И так мы повторяем до тех пор, пока изменения нашего вектора не будет меньше, чем некоторое заданное и очень маленькое значение $\epsilon$. Когда мы достигли предела то это значит что все, мы нашли первый собственный вектор, который соответствует наибольшему собственному значению. В частном случае интересных нам эрмитовых операторов, можно также последовательно находить все собственные вектора и собственные значения.

```{note}
На самом деле, наша сеть интернет является графом -- множеством связанных между собой вершин. А любой граф можно представить в виде большой-большой, но очень разреженной матрицы, каждый элемент которой это 1 если между соответствующими вершинами есть ребро и 0, если нет. Например, элемент $L_{ij}$ будет 1, если между вершинами $i$ и $j$ есть ребро. В 1998-м году, Ларри Пейдж и Сергей Брин нашли очень эффективный способ как можно посчитать первый собственный вектор этой матрицы используя именно модификацию степенного метода. Этот алгоритм получил название `PageRank`, причем `Page` это именно фаимилия автора, а не отсылка к веб-страницам. И именно этот алгоритм лег в основу поисковика _Goole_, который в дальнейшем вырос в транснациональную корпорацию!
```

### Итерация Арнольди

Гораздо более сложный метод, который, однако, является одним из самых эффективных применительно к разреженным матрицам. Объяснить его легко, к сожалению, не получится, так как алгоритм требует понимания Крыловских подпространств и других концептов из области линейной алгебре разреженных систем. Но нам достаточно лишь того, что этот алгоритм имеет очень эффективную реализацию -- [ARPACK](https://www.caam.rice.edu/software/ARPACK/), написанную в середине 90-х годов на языке `FORTRAN77`. Именно эта библиотека используется "под капотом" у `SciPy`, а также во многих других научных пакетах. Давайте посмотрим, как она работает.

Сгенерируем большую разреженную матрицу.

```{code-cell} ipython3
import numpy as np
from scipy import sparse

np.random.seed(42)
x = np.random.random(10000)
np.random.seed(42)
y = np.random.random(10000)

px = np.where(x > 0.2)
py = np.where(y > 0.2)

num_elements = max([px[0].shape[0], py[0].shape[0]])

spmat = sparse.coo_matrix(
    (
        (np.ones(num_elements),
        (px[0][:num_elements], py[0][:num_elements]))
    )
)
print(spmat.__repr__())
```

Матрица размера $10000 \times 10000$ это большая матрица и работать с ней в "плотном" (dense) представлении было бы очень трудно. Но `ARPACK` позволит нам найти минимальное собственное значение за доли секунд используя разреженность матрицы:

```{code-cell} ipython3
from scipy.sparse import linalg as sl

max_eigval = sl.eigs(spmat, k=1, which="LR", return_eigenvectors=False)[0]
min_eigval = sl.eigs(spmat, k=1, which="SR", return_eigenvectors=False)[0]

print(f"Min E: {min_eigval}\nMax E: {max_eigval}")
```

```{note}
Не у всех матриц все собственные значения являются действительными, поэтому `ARPACK` по умолчанию считает нам комплексные значения, хотя в этом конкретном случае мы видим, что мнимая часть равна нулю.
```

В дальнейшем мы будем использовать `ARPACK`, вызывая его из `SciPy` для проверки того, что наш вариационный квантовый алгоритм нашел верное решение задачи.

## VQE

Теперь давайте посмотрим, как эта задача связана с квантовым компьютером, а также как ее можно свести к градиентному спуску в пространстве параметров `VQE`, содержащей лишь гейты вращения. Напомню, что наличие лишь гейтов вращения [позволяет нам эффективно считать градиенты](gradients).

### Собственные значения как результат измерения

Итак, у нас есть эрмитова матрица, для которой мы хотим найти минимальное собственное значение. То есть сделать примерно то, что делает `ARPACK`, но на квантовом компьютере.

```{note}
Если наша матрица $A$ не эрмитова, то из линейной алгебры мы точно знаем, что мы всегда можем дополнить ее до эрмитовой. Поэтому далее, без потери общности, мы будем считать эту матрицу именно эрмитовой.
```

Из первых лекций мы помним, что любое измерение квантового оператора в каком-то состоянии [есть проекция этого состояния на пространство его собственных векторов](../qcblock/qubit.html#id31), а результатом измерения [является одно из его собственных значений](../qcblock/qubit.html#id25). При этом распределение вероятностей получения этих собственных значений определяется нашей волновой функцией $\ket{\Psi}$ измеряемого состояния. А значит, то, какое собственное значение мы получим будет определяться тем, какое состояние мы приготовим! Более того, мы помним, что минимальное значения энергии, или минимальное собственное значение в терминах нашей задачи является наиболее вероятным результатом измерения основного состояния. Напомню, что мы записываем результат измерения оператора $\hat{A}$ в состоянии $\ket{\Psi}$ так:

$$
e = \bra{\Psi}\hat{A}\ket{\Psi}
$$

А, как мы помним из [лекции по параметризированным квантовым схемам](vqc) то, какое состояние мы приготовим мы можем задать собственно самой схемой (еще говорят _ansatz_), а также набором ее классических параметров, которые мы можем варьировать.

Так мы приходим к алгоритму `VQE` -- **V**ariational **Q**uantum **E**igensolver:

1. Выбираем _ansatz_ -- оператор $\hat{B}$, который представляет из себя последовательность гейтов вращения
2. Генерируем случайные начальные параметры $q_0$
3. Цикл обучения, повторяем $n$ раз
   1. Готовим состояние $\ket{\Psi} = \hat{B}(q_n)\ket{0}$
   2. Измеряем $e = \bra{\Psi}\hat{A}\ket{\Psi}$
   3. Вычисляем градиент $g = \frac{\partial{e}}{\partial{q}}$
   4. Обновляем параметры $q_{n+1} = q_n - \alpha g$
4. Результатом обучения являются:
   - Набор параметров $q$ такой, что $\hat{B}(q)\ket{0}$ является собственным вектором, близким к первому собственному вектору $\hat{A}$
   - Результат измерения $e$, близкий к первому собственному значению $\hat{A}$

## Гамильтониан Изинга

Хорошо, ну умеем мы находить первое собственное значение большой эрмитовой матрицы и соответствующий собственный вектор. А что нам с этого? Почему это так важно и этот алгоритм такой крутой?

На самом деле ответ в том, что огромное число задач реального мира, включая такие направления, как логистика, квантовая химия и даже анализ социальных сетей можно свести к задаче нахождения минимальной энергии специальной матрицы. Причем эта матрица ну прямо очень хорошо моделируется на квантовом компьютере! Это так называемый оператор Изинга или гамильтониан Изинга.

```{note}
Специальные квантовые компьютеры компании D-Wave сконструированы так, что они могут решать вообще только одну задачу -- нахождения основного состояния гамильтонианов типа Изинга. Но эта задача настолько распространена и важна, что эти компьютеры стали первыми в мире коммерческими квантовыми компьютерами! Кстати далее этим компьютерам у нас будет посвящена отдельная лекция.
```

Далее мы посвятим довольно много времени объяснению этой модели. Это может показаться скучным и занудным, но это важно для понимания того, как это все работает и как решать с помощью `VQE` реальные задачи!

### Задача Изинга в одномерном случае

```{note}
Дальше мы попробуем на пальцах объяснить модель Изинга. Пробовать мы будем через цепочку атомов ферромагнетика во внешнем магнитном поле. Ели вы плохо помните физику и вам это объяснение покажется сложным, то не расстраивайтесь -- дальше мы также объясним задачу Изинга как задачу о поиске максимального разреза в графе -- известную задачу комбинаторной оптимизации.
```

Пусть у нас есть, например, цепочка атомов, которые обладают магнитным моментом. Например, цепочка атомов ферромагнетика, такого как железо. И мы прикладываем к этой цепочке внешнее магнитное поле.

Тогда, если поле маленькое, то наши атомы будут стараться выстроиться в антиферромагнитный порядок, когда соседние из них имеют моменты, направленные в разные стороны. Но если поле уже большое, то оно будет стремиться "повернуть" моменты по своему направлению. А если еще вспомнить, что магнитный момент атома является квантовой величиной и может быть в суперпозиции состояний в одну сторону и в противоположную, то не очень маленькое, но и не слишком большое поле будет переводить часть атомов именно в такие суперпозиции.

Давайте теперь запишем Гамильтониан такой системы. Для представления магнитных моментов мы будем использовать оператор $\sigma^z$ -- другими словами, спин в направлении оси $Z$. Если кто-то забыл, как выглядит оператор $\sigma^z$, то рекомендуем еще раз просмотреть [раздел про операторы Паули первой лекции](../qcblock/qubit.html#id24). Далее мы будем очень активно использовать эти матрицы для представления задач реального мира!

Для начала, в случае если внешнего поля нет, то мы должны записать взаимодействие соседних атомов. Так как у нас ферромагнетик, то минимальная энергия достигается в случае, если у нас спины противонаправленны. Это просто оператор $\sigma^z_j\sigma^z_j$, который действует на все пары соседних спинов. Ну и сразу введем некоторую константу обменного взаимодействия $J$, чтобы потом нам было удобно сравнивать ее с внешним полем. В итоге, для цепочки из $N$ спинов, получаем:

$$
\hat{H_{h=0}} = J \sum_{i=0}^{N-1} \sigma^z_i\sigma^z_{i+1}
$$

А теперь давайте добавим внешнее поле $h$. В этом случае, поле просто действует на все спины и пытается выстроить их, в зависимости от своего направления, например вниз. Тогда полный гамильтониан такой системы можно записать в виде:

$$
\hat{H_{h\neq 0}} = J \sum_{i=0}^{N-1} \sigma^z_i\sigma^z_{i+1} - h\sum_{i=0}^N \sigma^z_i
$$

### Задача Изинга как задача о максимальном разрезе в графе

Задача о максимальном разрезе в графе это очень известная задача комбинаторики. Она относится к классу $NP$-трудных и к ней можно свести все другие $NP$ задачи. При этом ее формулировка одна из самых простых среди всего класса задач. Формулируется она следующим образом.

Нам дан граф -- набор вершин $V$ и связывающих их ребер $E$. Нам надо найти такое разделение ребер $V$ на два не пересекающихся набора $V_1, V_2$, что число ребер между вершинами из разных наборов будет максимально.

```{figure} /_static/vqeblock/vqe/Max-cut.png
:width: 400px
:name: MaxCut

Иллюстрация задачи о максимальном разрезе в графе
```

Теперь давайте представим, что наши вершины графа мы моделируем спинами (ну или кубитами): спин "вверх" это, например, $V_1$, а спин "вниз" это $V_2$. Ну и для простоты предположим, что наш граф это просто цепочка, то есть ребра есть лишь между соседними в одномерном пространстве вершинами. Тогда легко записать функцию стоимости:

$$
H = \sum_{i=0}^{N-1} \sigma^z_i\sigma^z_{i+1}
$$

Ну и теперь давайте представим, что наша задачу чуточку сложнее -- нам надо найти не просто максимальный разрез, а такой разрез, который самый большой при наименьшем числе вершин в наборе $V_1$. Ну и, поскольку теперь у нас два вклада в стоимость, то нам нужны коэффициенты, которые покажут, что нам важнее. Пусть это будут $J$ и $h$. Финальная стоимость:

$$
H = J \sum_{i=0}^{N-1} \sigma^z_i\sigma^z_{i+1} - h\sum_{i=0}^N \sigma^z_i
$$

Мы и так как у нас собственные значения $\sigma^z$ это как раз $\pm 1$, то мы совершенно справедливо можем записать это все в виде Гамильтониана:

$$
\hat{H} = J \sum_{i=0}^{N-1} \sigma^z_i\sigma^z_{i+1} - h\sum_{i=0}^N \sigma^z_i
$$

Как видно, это тот же самый гамильтониан, который мы получили и для моделирования ферромагнетиков.

### Модель Изинга на чистом NumPy

Давайте попробуем реализовать гамильтониан Изинга на чистом `NumPy`/`SciPy` в виде разреженной матрицы. Для этого вспомним [из первых лекций](qubit), что действуя оператором $\sigma^z$ на $i$ кубит мы одновременно действуем единичным оператором на все остальные, а потом перемножаем все операторы произведением Кронекера. Из [лекций по линейной алгебре](пусто) мы помним также об ассоциативности произведения Кронекера, чем и воспользуемся:

```{code-cell} ipython3
def sigmaz_k(k, n):
    left_part = sparse.eye(2 ** k)
    right_part = sparse.eye(2 ** (n - 1 - k))

    return sparse.kron(
        sparse.kron(
            left_part,
            sparse.csr_matrix(np.array([[1, 0,], [0, -1,],]))
        ),
        right_part
    )
```

Ну а теперь мы можем реализовать и сам оператор Изинга:

```{code-cell} ipython3
def ising(j, h, n):
    res = sparse.csr_matrix((2 ** n, 2 ** n), dtype=np.complex)

    for i in range(n - 1):
        res += j * sigmaz_k(i, n) * sigmaz_k(i + 1, n)
        res -= h * sigmaz_k(i, n)

    res -= h * sigmaz_k(n - 1, n)

    return res
```

Если внешнего поля нет, то наши спины выстраиваются в полный антиферромагнитный порядок, в чем легко убедиться. Создадим оператор для такой модели и, например, 10 спинов (или 10 вершин в графе, если мы говорим в терминах Max-Cut):

```{code-cell} ipython3
op = ising(1, 0, 10)
solution = sl.eigs(op, which="SR", k=1, return_eigenvectors=True)
print(f"Energy: {solution[0][0]}")
```

Эта энергия соответствует антиферромагнитному порядку, в этом легко убедиться, нарисовав спины и формулу на бумажке. Внимательный читатель заметил, что в этот раз мы вернули также и первый собственный вектор, который в нашем случае является волновой функцией основного состояния. А, как мы помним, квадраты элементов вектора волновой функции дают нам вероятности соответствующих битовых строк (если для вас это все звучит дико, то очень рекомендуем вернуться к [лекции про кубит](qubit)). Давайте посмотрим на эту битовую строку, или, другими словами, на порядок наших спинов в решении (или на разбиение вершин графа на два подмножества в терминах Max-Cut):

```{code-cell} ipython3
probs = solution[1] * solution[1].conj()
bit_s_num = np.where(probs == probs.max())[0][0]
s = f"{bit_s_num:b}"
s = "0" * (10 - len(s)) + s

print(s)
```

Теперь давайте попробуем добавить внешнее поле с коэффициентом, равным удвоенному значению константы обменного взаимодействия. В терминах комбинаторной задачи, мы добавляем штраф, равный два умножить на число спинов, направленных вверх.

```{code-cell} ipython3
op = ising(1, 2, 10)
solution = sl.eigs(op, which="SR", k=1, return_eigenvectors=True)
print(f"Energy: {solution[0][0]}")

probs = solution[1] * solution[1].conj()
bit_s_num = np.where(probs == probs.max())[0][0]
s = f"{bit_s_num:b}"
s = "0" * (10 - len(s)) + s

print(s)
```

Видим, что теперь наш антиферромагнитный порядок уже не полный. В целом, данная модель довольно интересная, так как при некотором отношении $\frac{h}{J}$ у нас происходит фазовый переход от полной упорядоченности, а при дальнейшем росте $h$ мы приходим к полностью одинаковой ориентации всех спинов, в чем легко убедиться, взяв, например, $h = 100$:

```{code-cell} ipython3
op = ising(1, 100, 10)
solution = sl.eigs(op, which="SR", k=1, return_eigenvectors=True)
print(f"Energy: {solution[0][0]}")

probs = solution[1] * solution[1].conj()
bit_s_num = np.where(probs == probs.max())[0][0]
s = f"{bit_s_num:b}"
s = "0" * (10 - len(s)) + s

print(s)
```

## Ну и причем тут _NP_-трудные задачи?

Мы потратили столько времени разбираясь с этой сложной антиферромагнитной моделью, а также совершенно оторванной от мира задачей о максимальном разрезе в графе. Теперь мы наконец-то увидим, что все это было не зря!

### Формулировка задач комбинаторной оптимизации в терминах гамильтониана Изинга

Далее мы приведем примеры сведения некоторых интересных задач оптимизации к проблеме Изинга. Все эти примеры мы будем брать из публикации {cite}`combinatorics2ising`. И перед тем, как мы начнем, нам понадобиться ввести еще одно определение, а именно оператор, у которого собственные вектора совпадают с таковыми у $\sigma^z$, а собственные значения равны 0 или 1 (вместо $\pm1$ для $\sigma^z$):

$$
\hat{x} = (I + \sigma^z) / 2 = \begin{vmatrix} 1 & 0 \\
0 & 0 \end{vmatrix}
$$

####

## Решение задачи коммивояжера с помощью VQE

## Заключение
